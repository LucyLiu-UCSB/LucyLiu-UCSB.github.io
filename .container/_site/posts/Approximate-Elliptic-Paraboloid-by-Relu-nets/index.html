<!DOCTYPE html><html lang="en"> <!-- The Head © 2017-2019 Cotes Chung MIT License --><head><title>Approximating Elliptic Paraboloid by Relu nets | Lucy's Blog</title><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v3.8.6" /><meta property="og:title" content="Approximating Elliptic Paraboloid by Relu nets" /><meta name="author" content="Lucy Liu" /><meta property="og:locale" content="en_US" /><meta name="description" content="If we regard neural net models as nonparametric approximators of continuous functions, several works are supporting the validity of this method. In particular, in reference 1, it has been proved that a one hidden layer neural net with sigmoid activation function can approximate any continuous functions on a unit cube. And later, there are works showing this is also true for Relu activation. The conclusion here is a large flat net is safe to approximate any continuous function. in reference 2, oppositely, it shows that with hidden layer widths at most \(w\), and arbitrary depth, Relu nets can approximate any countinous functions. If the net has input dimension \(d_{input}\) and output dimension \(d_{out}\), \(w\) should satisfy \[d_{input}+1 \leq w \leq d_{input}+d_{out}.\] In other words, if all hidden layer widths are bounded by \(d_{input}\), even in the infinite depth limit, Relu nets can only express a very limited class of functions." /><meta property="og:description" content="If we regard neural net models as nonparametric approximators of continuous functions, several works are supporting the validity of this method. In particular, in reference 1, it has been proved that a one hidden layer neural net with sigmoid activation function can approximate any continuous functions on a unit cube. And later, there are works showing this is also true for Relu activation. The conclusion here is a large flat net is safe to approximate any continuous function. in reference 2, oppositely, it shows that with hidden layer widths at most \(w\), and arbitrary depth, Relu nets can approximate any countinous functions. If the net has input dimension \(d_{input}\) and output dimension \(d_{out}\), \(w\) should satisfy \[d_{input}+1 \leq w \leq d_{input}+d_{out}.\] In other words, if all hidden layer widths are bounded by \(d_{input}\), even in the infinite depth limit, Relu nets can only express a very limited class of functions." /><link rel="canonical" href="http://localhost:4000/posts/Approximate-Elliptic-Paraboloid-by-Relu-nets/" /><meta property="og:url" content="http://localhost:4000/posts/Approximate-Elliptic-Paraboloid-by-Relu-nets/" /><meta property="og:site_name" content="Lucy’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-01-14T23:34:00-08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Approximating Elliptic Paraboloid by Relu nets" /><meta name="twitter:site" content="@LucyLiu84049511" /><meta name="twitter:creator" content="@Lucy Liu" /><meta name="google-site-verification" content="828U5Odxo38lGZ7ca6xyIc_r3oCovf3Xsr-XyShs3Ek" /> <script type="application/ld+json"> {"description":"If we regard neural net models as nonparametric approximators of continuous functions, several works are supporting the validity of this method. In particular, in reference 1, it has been proved that a one hidden layer neural net with sigmoid activation function can approximate any continuous functions on a unit cube. And later, there are works showing this is also true for Relu activation. The conclusion here is a large flat net is safe to approximate any continuous function. in reference 2, oppositely, it shows that with hidden layer widths at most \\(w\\), and arbitrary depth, Relu nets can approximate any countinous functions. If the net has input dimension \\(d_{input}\\) and output dimension \\(d_{out}\\), \\(w\\) should satisfy \\[d_{input}+1 \\leq w \\leq d_{input}+d_{out}.\\] In other words, if all hidden layer widths are bounded by \\(d_{input}\\), even in the infinite depth limit, Relu nets can only express a very limited class of functions.","@type":"BlogPosting","headline":"Approximating Elliptic Paraboloid by Relu nets","dateModified":"2020-01-14T23:34:00-08:00","datePublished":"2020-01-14T23:34:00-08:00","url":"http://localhost:4000/posts/Approximate-Elliptic-Paraboloid-by-Relu-nets/","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/Approximate-Elliptic-Paraboloid-by-Relu-nets/"},"author":{"@type":"Person","name":"Lucy Liu"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/main.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script> document.jQuery || document.write('<script src="/assets/lib/jquery-3.4.1.min.js"><\/script>'); </script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.15.0/dist/umd/popper.min.js" integrity="sha256-fTuUgtT7O2rqoImwjrhDgbXTKUwyxxujIMRIK7TbuNU=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha256-5+02zu5UULQkO7w1GIr6vftCgMfFdZcAHeDtFnKZsBs=" crossorigin="anonymous"></script> <script src="/assets/js/dist/commons.js" async></script> <script src="/assets/js/dist/timeago.min.js"></script><link rel="stylesheet" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/lib/bootstrap-toc-1.0.1/bootstrap-toc.min.css" /> <script src="/assets/lib/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script> <script src="/assets/js/dist/toc.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar © 2017-2019 Cotes Chung MIT License --><div id="nav-wrap"><div id="profile-wrap" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/Lucy.png"></img> </a></div><div class="profile-text mt-3"><div id="site-title"> <a href="/">Lucy's Blog</a></div><div id="site-subtitle" class="font-italic">Learning for the sake of Learning</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-3 mr-4 hidden"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-3 mr-4 hidden"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-3 mr-4 hidden"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-3 mr-4 hidden"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-3 mr-4 hidden"></i> <span>ABOUT</span> </a></li></ul></div><div class="contact d-flex justify-content-around mt-4"> <a href="https://github.com/LucyLiu-UCSB" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/LucyLiu84049511" target="_blank"> <i class="fab fa-twitter"></i> </a> <a href="javascript:window.open('mailto:' + ['xiliu','ucsb.edu'].join('@'))"> <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" target="_blank"> <i class="fas fa-rss"></i> </a></div></div><div id="main-wrap"> <!-- The Top Bar © 2017-2019 Cotes Chung MIT License --><div id="topbar" class="bg-white row justify-content-center topbar-down"><div id="topbar-main" class="d-flex h-100 align-items-center justify-content-between col-12 col-md-12 col-lg-11 col-xl-11 pl-md-2 pr-md-2 pl-lg-2 pr-lg-2"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Approximating Elliptic Paraboloid by Relu nets</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrap"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> </span> <a href="javascript:;">Cancel</a></div></div><div id="main"><div class="row justify-content-center bg-white"> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <!-- Define the liquid date formats. © 2019 Cotes Chung Published under the MIT License --><div class="col-12 col-lg-11 col-xl-8"><div id="post-wrap" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4 pl-xl-3"><div class="post"><h1 data-toc-skip>Approximating Elliptic Paraboloid by Relu nets</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago" data-toggle="tooltip" title="Tue, Jan 14, 2020, 11:34 PM -0800"> Jan 14, 2020 <i class="hidden">2020-01-14T23:34:00-08:00</i> </span> on <a href='/categories/deep-learning/'>Deep learning</a>, <a href='/categories/theoretical-exploration/'>Theoretical exploration</a></div></div><div class="post-content"><p>If we regard neural net models as nonparametric approximators of continuous functions, several works are supporting the validity of this method. In particular,</p><ul><li>in reference 1, it has been proved that a one hidden layer neural net with sigmoid activation function can approximate any continuous functions on a unit cube. And later, there are works showing this is also true for Relu activation. The conclusion here is a large flat net is safe to approximate any continuous function.</li><li>in reference 2, oppositely, it shows that with hidden layer widths at most \(w\), and arbitrary depth, Relu nets can approximate any countinous functions. If the net has input dimension \(d_{input}\) and output dimension \(d_{out}\), \(w\) should satisfy \[d_{input}+1 \leq w \leq d_{input}+d_{out}.\] In other words, if all hidden layer widths are bounded by \(d_{input}\), even in the infinite depth limit, Relu nets can only express a very limited class of functions.</li></ul><p>In the following, we will see how Relu nets approximate a Elliptic Paraboloid function with different depth and width.</p><p>\[z = 10*(x-2)^2+10*(y-2)^2, x, y \in(0, 4)\times(0, 4)\]</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">'seaborn-whitegrid'</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">10</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
<span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
</pre></table></code></div></div><p><img src="/assets/img/sample/ellip_20200114.png" alt="fdr" width="300" class="center" /></p><h2 id="model-1-linear-regression-model">Model 1: Linear regression model</h2><p>The basic linear regression model fits a linear function, \[z\sim w_1*x+w_2*y + b.\]</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">linearModel</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'normal'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s">'dense1'</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">linearModel</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
                   <span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span>
                   <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>

<span class="n">fithistory_linear</span> <span class="o">=</span> <span class="n">linearModel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xmat</span><span class="p">,</span> <span class="n">zvec</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">fittedlinear</span> <span class="o">=</span> <span class="n">linearModel</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xmat</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
</pre></table></code></div></div><p>Obviously, the fitted \(z\) will just be a plane as in the following figure.</p><p><img src="/assets/img/sample/linearplot_20200114.png" alt="fdr" width="300" align="center" /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="n">Model</span><span class="p">:</span> <span class="s">"linearModel"</span>
<span class="n">________________________________________</span>
<span class="n">Layer</span> <span class="p">(</span><span class="nb">type</span><span class="p">)</span>      <span class="n">Output</span> <span class="n">Shape</span>    <span class="n">Param</span> 
<span class="o">========================================</span>
<span class="n">dense1</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span>    <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>       <span class="mi">3</span>     
<span class="o">========================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">3</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">3</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">________________________________________</span>
</pre></table></code></div></div><h2 id="model-2-1-hidden-layer-with-2040-units">Model 2: 1 hidden layer with 20/40 units</h2><p>As in reference 1, we fit a shallow net with 1 hidden layer having 20/40 units. Even with dropout, the training process is not stable. After several trials, the loss function still can not reach the global minimum. As shown in the plot below, the blue fitted surface could have been closer to the truth. Also, the model has 81/161 parameters.</p><p>If we set the hidden layer has 20 units, since each Relu activation function folds the plane once, 20 Relu activations fold 20 planes which leads to a surface-wise constant function with 21 faces.</p><p><img src="/assets/img/sample/hidden1_w20_20200114.png" alt="fdr" width="600" align="center" /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="n">Model</span><span class="p">:</span> <span class="s">"hidden1_width_20/40"</span>
<span class="n">______________________________________________________</span>
<span class="n">Layer</span> <span class="p">(</span><span class="nb">type</span><span class="p">)</span>              <span class="n">Output</span> <span class="n">Shape</span>        <span class="n">Param</span> <span class="c1"># 
</span><span class="o">======================================================</span>
<span class="n">dense_2_to_20</span><span class="o">/</span><span class="mi">40</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">20</span><span class="o">/</span><span class="mi">40</span><span class="p">)</span>          <span class="mi">60</span><span class="o">/</span><span class="mi">120</span>   
<span class="n">______________________________________________________</span>
<span class="n">re_lu</span> <span class="p">(</span><span class="n">ReLU</span><span class="p">)</span>             <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">20</span><span class="o">/</span><span class="mi">40</span><span class="p">)</span>          <span class="mi">0</span>       
<span class="n">______________________________________________________</span>
<span class="n">dropout</span> <span class="p">(</span><span class="n">Dropout</span><span class="p">)</span>        <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">20</span><span class="o">/</span><span class="mi">40</span><span class="p">)</span>          <span class="mi">0</span>       
<span class="n">______________________________________________________</span>
<span class="n">dense_20</span><span class="o">/</span><span class="mi">40</span><span class="n">_to_1</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>              <span class="mi">21</span><span class="o">/</span><span class="mi">41</span>      
<span class="o">======================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">81</span><span class="o">/</span><span class="mi">161</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">81</span><span class="o">/</span><span class="mi">161</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">______________________________________________________</span>
</pre></table></code></div></div><h2 id="model-3-2-hidden-layers-with-10-units">Model 3: 2 hidden layers with 10 units</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">hidden_2_width_10</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'normal'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s">'dense_2_to_10'</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span>  <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'normal'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s">'dense_10_to_10'</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'normal'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s">'dense_10_to_1'</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">hidden_2_width_10</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">),</span>
                   <span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span>
                   <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>

<span class="n">fithistory_hidden_2_width_10</span> <span class="o">=</span> <span class="n">hidden_2_width_10</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xmat</span><span class="p">,</span> <span class="n">zvec</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</pre></table></code></div></div><p><img src="/assets/img/sample/hidden2_w10_20200114.png" alt="fdr" width="400" align="center" /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre><td class="rouge-code"><pre><span class="n">Model</span><span class="p">:</span> <span class="s">"hidden2_width_10"</span>
<span class="n">_________________________________________________________________</span>
<span class="n">Layer</span> <span class="p">(</span><span class="nb">type</span><span class="p">)</span>                 <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   
</span><span class="o">=================================================================</span>
<span class="n">dense_2_to_10</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span>        <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                <span class="mi">30</span>        
<span class="n">_________________________________________________________________</span>
<span class="n">re_lu_60</span> <span class="p">(</span><span class="n">ReLU</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                <span class="mi">0</span>         
<span class="n">_________________________________________________________________</span>
<span class="n">dropout_37</span> <span class="p">(</span><span class="n">Dropout</span><span class="p">)</span>         <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                <span class="mi">0</span>         
<span class="n">_________________________________________________________________</span>
<span class="n">dense_10_to_10</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span>       <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                <span class="mi">110</span>       
<span class="n">_________________________________________________________________</span>
<span class="n">re_lu_61</span> <span class="p">(</span><span class="n">ReLU</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                <span class="mi">0</span>         
<span class="n">_________________________________________________________________</span>
<span class="n">dropout_38</span> <span class="p">(</span><span class="n">Dropout</span><span class="p">)</span>         <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>                <span class="mi">0</span>         
<span class="n">_________________________________________________________________</span>
<span class="n">dense_10_to_1</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span>        <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                 <span class="mi">11</span>        
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">151</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">151</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">_________________________________________________________________</span>
</pre></table></code></div></div><p>Though we have 151 parameters, but the fitted surface is far away from the truth, which is even worse than 1 hidden layer with 20 units (81 parameters).</p><h2 id="model-4-3-hidden-layers-with-5-units">Model 4: 3 hidden layers with 5 units</h2><p>If we further make the Relu nets deeper to have 3 hidden layers, and each layer has 5 units, this will result in a net with 81 parameters. However, the final model is close to a purely linear model. Further investigation of the weight matrix reveals that all the fitted weights are much smaller than the weight in the 1 hidden layer models. Therefore, when the model is deeper, there are more plateaus in the objective function which makes the training very hard.</p><p><img src="/assets/img/sample/hidden3_w5_20200114.png" alt="fdr" width="400" align="center" /></p><h2 id="model-5-1-hidden-5-units--batchnormalization">Model 5: 1 hidden 5 units + BatchNormalization</h2><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="n">hidden_1_width_5_BN</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'normal'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s">'dense_2_to_5'</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'normal'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s">'dense_5_to_1'</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">hidden_1_width_5_BN</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
                   <span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span>
                   <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>

<span class="n">fithistory_hidden_1_width_5_BN</span><span class="o">=</span> <span class="n">hidden_1_width_5_BN</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xmat</span><span class="p">,</span> <span class="n">zvec</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</pre></table></code></div></div><p><img src="/assets/img/sample/batch_20200114.png" alt="fdr" width="400" align="center" /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="n">Model</span><span class="p">:</span> <span class="s">"hidden_1_width_5_BN"</span>
<span class="n">_________________________________________________________________</span>
<span class="n">Layer</span> <span class="p">(</span><span class="nb">type</span><span class="p">)</span>                 <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   
</span><span class="o">=================================================================</span>
<span class="n">dense_2_to_5</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span>         <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>                 <span class="mi">15</span>        
<span class="n">_________________________________________________________________</span>
<span class="n">batch_normalization</span> <span class="p">(</span><span class="n">Batch</span><span class="p">)</span>  <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>                 <span class="mi">20</span>        
<span class="n">_________________________________________________________________</span>
<span class="n">re_lu</span> <span class="p">(</span><span class="n">ReLU</span><span class="p">)</span>                 <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>                 <span class="mi">0</span>         
<span class="n">_________________________________________________________________</span>
<span class="n">dense_5_to_1</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span>         <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                 <span class="mi">6</span>         
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">41</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">31</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">10</span>
<span class="n">_________________________________________________________________</span>
</pre></table></code></div></div><p>If we only have 1 hidden layer with only 5 units, adding the batch normalization not only enables convergence faster but also results in an excellent fit. Also, the total number of trainable parameters is 31, which is the smallest among all the above models.</p><h2 id="loss-function-in-training-stage">Loss function in training stage</h2><p><img src="/assets/img/sample/loss_20200114.png" alt="fdr" width="800" align="center" /></p><p>According to this simple post, models with batch normalization work the best even with the least number of parameters. Batch normalization not only accelerates the optimization procedure but also leads to an appropriate modeling structure. A more theoretical investigation of the batch normalization is necessary.</p><h2 id="references">References</h2><ol><li><a href="https://www.researchgate.net/profile/George_Cybenko/publication/226439292_Approximation_by_superpositions_of_a_sigmoidal_function_Math_Cont_Sig_Syst_MCSS_2303-314/links/551d50c90cf23e2801fe12cf/Approximation-by-superpositions-of-a-sigmoidal-function-Math-Cont-Sig-Syst-MCSS-2303-314.pdf">G.Cybenko, Approximation by superpositions of a sigmoidal function (1989)</a></li><li><a href="https://arxiv.org/abs/1710.11278">Boris Hanin, Mark Sellke, Approximating Continuous Functions by ReLU Nets of Minimal Width (2018)</a></li></ol></div><div class="post-tail text-muted"><div class="mb-4"> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >Deep learning</a></div></div></div><!-- The related posts of current post. Placed in the bottom of every single post. © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-4 mb-4 pb-3"><h3 class="pt-2 mt-1 mb-4" data-toc-skip>Related Posts</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Theoertical-investigation-of-batch-normalization/"><div class="card-body"> <span class="timeago small"> Jan 17, 2020 <i class="hidden">2020-01-17T08:25:00-08:00</i> </span><h3 class="pt-0 mt-2 mb-3" data-toc-skip>Theoretical investigation of batch normalization</h3><div class="text-muted small"><p>In last post, models with batch normalization have show advantages by a large margin. In this post, we will explore the following questions: How does batch normalization work? How is batch no...</p></div></div></a></div><div class="card"> <a href="/posts/Backpropagation-of-a-vanilla-RNN/"><div class="card-body"> <span class="timeago small"> Jan 24, 2020 <i class="hidden">2020-01-24T23:11:00-08:00</i> </span><h3 class="pt-0 mt-2 mb-3" data-toc-skip>Backpropagation of a vanilla RNN</h3><div class="text-muted small"><p>This post investigates how to code up a vanilla RNN. Most of the code and example are copied from Andrej Karpathy’s blog: The Unreasonable Effectiveness of Recurrent Neural Networks. 100-line ...</p></div></div></a></div></div></div><div class="post-pager d-flex justify-content-between"> <a href="/posts/Multiple-comparisons/" class="btn btn-outline-primary"> <i class="fas fa-angle-left mr-1"></i> OLDER POST </a> <a href="/posts/Theoertical-investigation-of-batch-normalization/" class="btn btn-outline-primary"> NEWER POST <i class="fas fa-angle-right ml-1"></i> </a></div><!-- The Disqus lazy loading. Powered by: https://osvaldas.info/lazy-loading-disqus-comments © 2019 Cotes Chung MIT License --><div id="disqus" class="pt-2 pb-4"><p class="font-italic text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//LucyLiu.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'http://localhost:4000/posts/Approximate-Elliptic-Paraboloid-by-Relu-nets/'; this.page.identifier = '/posts/Approximate-Elliptic-Paraboloid-by-Relu-nets/'; } }; $.disqusLoader('#disqus', options); </script></div></div><!-- The Pannel on right side (Desktop views) © 2017-2019 Cotes Chung MIT License --><div id="panel-wrap" class="col-xl-3 pl-2 topbar-down"><div class="access"><div id="access-lastmod" class="post mb-4""><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Theoertical-investigation-of-batch-normalization/">Theoretical investigation of batch normalization</a></li><li><a href="/posts/Some-useful-operations-in-MySQL-median-level/">Some useful operations in MySQL--Median level</a></li><li><a href="/posts/Multiple-comparisons/">Multiple comparisons</a></li><li><a href="/posts/Sort-multiple-variables/">Sort multiple variables</a></li><li><a href="/posts/Some-useful-operations-in-MySQL-easy-level/">Some useful operations in MySQL--Easy level</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/manifold/">Manifold</a> <a class="post-tag" href="/tags/embedding/">Embedding</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/sql/">SQL</a> <a class="post-tag" href="/tags/recursion/">Recursion</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/statistics/">Statistics</a> <a class="post-tag" href="/tags/dynamic-programming/">Dynamic Programming</a> <a class="post-tag" href="/tags/divide&conquer/">Divide&Conquer</a> <a class="post-tag" href="/tags/directed-graph/">Directed Graph</a></div></div></div><div id="toc-wrap" class="pl-0 pr-4"><h3 data-toc-skip class="pl-3 pt-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><!-- The Footer © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="copyright"><p class="mb-0"> © 2019-2020<a href="https://twitter.com/LucyLiu84049511" class="ml-1">Lucy Liu</a>. <br>Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> & <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a>, hosted on <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.</p></div><div class="license"><p class="mb-0"> The blog posts on this site are licensed under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p></div></div></footer></div><!-- The Search results © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrap"><div class="row justify-content-center bg-white"><div class="col-12 col-md-12 col-lg-11 col-xl-9 pl-xl-5 pr-xl-5 pb-5 mt-3 mb-3"><h2 class="mt-3 pt-3 ml-3 ml-md-5 ml-lg-0" data-toc-skip>Search Results</h2><div class="post-content ml-1 ml-md-5 ml-lg-0"><ul id="search-results" ></ul></div></div></div></div></div><div id="mask"></div><!-- The Search © 2017-2019 Cotes Chung MIT License --> <script src="/assets/lib/simple-jekyll-search-1.7.1.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/search.json' }) </script> <a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a>
