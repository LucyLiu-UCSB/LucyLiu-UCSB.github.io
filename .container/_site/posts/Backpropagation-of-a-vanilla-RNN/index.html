<!DOCTYPE html><html lang="en"> <!-- The Head © 2017-2019 Cotes Chung MIT License --><head><title>Backpropagation of a vanilla RNN | Lucy's Blog</title><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v3.8.6" /><meta property="og:title" content="Backpropagation of a vanilla RNN" /><meta name="author" content="Lucy Liu" /><meta property="og:locale" content="en_US" /><meta name="description" content="This post investigates how to code up a vanilla RNN. Most of the code and example are copied from Andrej Karpathy’s blog: The Unreasonable Effectiveness of Recurrent Neural Networks. 100-line gist." /><meta property="og:description" content="This post investigates how to code up a vanilla RNN. Most of the code and example are copied from Andrej Karpathy’s blog: The Unreasonable Effectiveness of Recurrent Neural Networks. 100-line gist." /><link rel="canonical" href="http://localhost:4000/posts/Backpropagation-of-a-vanilla-RNN/" /><meta property="og:url" content="http://localhost:4000/posts/Backpropagation-of-a-vanilla-RNN/" /><meta property="og:site_name" content="Lucy’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-01-24T23:11:00-08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Backpropagation of a vanilla RNN" /><meta name="twitter:site" content="@LucyLiu84049511" /><meta name="twitter:creator" content="@Lucy Liu" /><meta name="google-site-verification" content="828U5Odxo38lGZ7ca6xyIc_r3oCovf3Xsr-XyShs3Ek" /> <script type="application/ld+json"> {"description":"This post investigates how to code up a vanilla RNN. Most of the code and example are copied from Andrej Karpathy’s blog: The Unreasonable Effectiveness of Recurrent Neural Networks. 100-line gist.","@type":"BlogPosting","headline":"Backpropagation of a vanilla RNN","dateModified":"2020-01-24T23:11:00-08:00","datePublished":"2020-01-24T23:11:00-08:00","url":"http://localhost:4000/posts/Backpropagation-of-a-vanilla-RNN/","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/Backpropagation-of-a-vanilla-RNN/"},"author":{"@type":"Person","name":"Lucy Liu"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/main.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script> document.jQuery || document.write('<script src="/assets/lib/jquery-3.4.1.min.js"><\/script>'); </script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.15.0/dist/umd/popper.min.js" integrity="sha256-fTuUgtT7O2rqoImwjrhDgbXTKUwyxxujIMRIK7TbuNU=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha256-5+02zu5UULQkO7w1GIr6vftCgMfFdZcAHeDtFnKZsBs=" crossorigin="anonymous"></script> <script src="/assets/js/dist/commons.js" async></script> <script src="/assets/js/dist/timeago.min.js"></script><link rel="stylesheet" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/lib/bootstrap-toc-1.0.1/bootstrap-toc.min.css" /> <script src="/assets/lib/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script> <script src="/assets/js/dist/toc.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar © 2017-2019 Cotes Chung MIT License --><div id="nav-wrap"><div id="profile-wrap" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/Lucy.png"></img> </a></div><div class="profile-text mt-3"><div id="site-title"> <a href="/">Lucy's Blog</a></div><div id="site-subtitle" class="font-italic">Learning for the sake of Learning</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-3 mr-4 hidden"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-3 mr-4 hidden"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-3 mr-4 hidden"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-3 mr-4 hidden"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-3 mr-4 hidden"></i> <span>ABOUT</span> </a></li></ul></div><div class="contact d-flex justify-content-around mt-4"> <a href="https://github.com/LucyLiu-UCSB" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/LucyLiu84049511" target="_blank"> <i class="fab fa-twitter"></i> </a> <a href="javascript:window.open('mailto:' + ['xiliu','ucsb.edu'].join('@'))"> <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" target="_blank"> <i class="fas fa-rss"></i> </a></div></div><div id="main-wrap"> <!-- The Top Bar © 2017-2019 Cotes Chung MIT License --><div id="topbar" class="bg-white row justify-content-center topbar-down"><div id="topbar-main" class="d-flex h-100 align-items-center justify-content-between col-12 col-md-12 col-lg-11 col-xl-11 pl-md-2 pr-md-2 pl-lg-2 pr-lg-2"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Backpropagation of a vanilla RNN</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrap"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> </span> <a href="javascript:;">Cancel</a></div></div><div id="main"><div class="row justify-content-center bg-white"> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <!-- Define the liquid date formats. © 2019 Cotes Chung Published under the MIT License --><div class="col-12 col-lg-11 col-xl-8"><div id="post-wrap" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4 pl-xl-3"><div class="post"><h1 data-toc-skip>Backpropagation of a vanilla RNN</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago" data-toggle="tooltip" title="Fri, Jan 24, 2020, 11:11 PM -0800"> Jan 24, 2020 <i class="hidden">2020-01-24T23:11:00-08:00</i> </span> on <a href='/categories/deep-learning/'>Deep learning</a>, <a href='/categories/theoretical-exploration/'>Theoretical exploration</a></div></div><div class="post-content"><p>This post investigates how to code up a vanilla RNN. Most of the code and example are copied from Andrej Karpathy’s blog:</p><ol><li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks.</a></li><li><a href="https://gist.github.com/karpathy/d4dee566867f8291f086">100-line gist.</a></li></ol><p>The backpropagation is clearly derived by the Chain rule in calculus.</p><h2 id="character-level-language-models">Character-Level Language Models</h2><p>For instance, we use the vanilla RNN to generate the next character in the world ‘‘hello’’, which is a character-level language model. The forward pass is illustrated in the following diagram. From the diagram, one can see that this RNN is a many-to-many model, where we have the one hot encoder of ‘h’, ‘e’, ‘l’, ‘l’ as input sequence and the four vectors of probability of ‘h’, ‘e’, ‘l’, ‘o’ as output. The loss function is the summation of 4 cross entropies since we totally make four predictions.</p><p><img src="/assets/img/sample/vanillaRNN_20200124.png" alt="fdr" width="700" class="center" /></p><h2 id="initialize-parameters">Initialize parameters</h2><p>As in the forward pass, we have three weight matrics, \(W_{xh}, W_{hh}, W_{hy}\) and two bias vectors, \(b_{h}, b_{y}\) , to learn. The weight matrics are randomly initialized and bias vectors are initialized to be 0.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">vanillaRNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="c1"># hyperparameters
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">n_x</span> <span class="o">=</span> <span class="n">n_x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_h</span> <span class="o">=</span> <span class="n">n_h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        
        <span class="c1"># initialize model parameters
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_x</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_h</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Why</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_h</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># memory vars for adagrad
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">mWxh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mWhh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Whh</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mWhy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Why</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mbh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bh</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mby</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">by</span><span class="p">)</span>

</pre></table></code></div></div><h2 id="forward-pass">Forward pass</h2><p>The simple RNN model replicates one neuron for <code class="highlighter-rouge">seq_length</code> times, with the same wight and bias parameter. The neuron has the previous hidden units and the new input as input information, and output the predicted <code class="highlighter-rouge">y</code> and the updated hidden units.</p><p>\[\begin{align} h_t &amp; = \text{tanh}(W_{hh}h_{t-1} + W_{xh}x_t +b_h) \\ y_t &amp; = W_{hy}h_{t} + b_y \\\ p_t &amp; = \frac{e^{y_t}}{\sum_{i = 1}^{n_y} e^{y_{t, i}}} \\\ l_t &amp; = -\sum_{i = 1}^{n_y} z_{t, i}\text{log}(p_{t, i}) \\\ L &amp; = \sum_{t = 1}^{n_{seq}}l_t \end{align}\]</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre>    <span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hprev</span><span class="p">):</span>
        <span class="s">"""
        inputs -- list of integers (tokenizer: char to int)
        targets -- list of integers (tokenizer: char to int)
        hprev -- the initial hidden state
        """</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>
        <span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hprev</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)):</span>
            
            <span class="c1"># one hot encoder of a char
</span>            <span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span> <span class="o">@</span> <span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span> <span class="o">@</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bh</span><span class="p">)</span>
            <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Why</span> <span class="o">@</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">by</span>
            <span class="n">p</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">targets</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="mi">0</span><span class="p">])</span>
            
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">p</span>
</pre></table></code></div></div><h2 id="backpropagation">backpropagation</h2><p>Backpropagation in the neural network is an application of chain rule in calculus. I write down the detailed derivation of the partial derivatives of the parameters.</p><p>\[dW_{hy} = \frac{\partial L}{\partial W_{hy}} = \sum_{t = 1}^{n_{seq}} \frac{\partial l_t}{W_{hy}},\text{ where }\] \[\begin{align} \frac{\partial l_t}{W_{hy}^{[i, :]}} &amp; = \frac{\partial l_t}{\partial p_{t, j}}\frac{\partial p_{t, j}}{y_{t, i}}\frac{\partial y_{t, i}}{\partial W_{hy}^{[i, :]}}, \text{ where } j = \arg_i\{z_{t, i} = 1\} \\\ &amp; = -\frac{1} {p_{t, j}} \left\{\begin{array}{lr} (p_{t, j} - p_{t, j}^2)h_t^T &amp; i = j \\ -(p_{t, i}p_{t,j})h_t^T &amp; i \neq j\end{array}\right. \end{align}\] Therefore, \[\begin{align}\frac{\partial l_t}{W_{hy}} &amp; = \frac{\partial l_t}{y_t}\cdot h_t^T = (p_t - e_j)\cdot h_t^T \\\ \frac{\partial l_t}{b_{y}} &amp; = \frac{\partial l_t}{y_t} = (p_t - e_j) \end{align}\]</p><p>\[\begin{align}dW_{hh} = \frac{\partial L}{\partial W_{hh}} &amp; = \sum_{t = 1}^{n_{seq}}\frac{\partial L}{\partial h_t}\frac{\partial h_t}{\partial W_{hh}}, \text{ set }h_{t}^{raw} = W_{hh}h_{t-1} + W_{xh}x_t +b_h \\\ &amp; = \sum_{t = 1}^{n_{seq}}\frac{\partial L}{\partial h_t}\frac{\partial h_t}{\partial h_{t}^{raw}}\frac{\partial h_t^{raw}}{\partial W_{hh}} \\\ &amp; = \sum_{t = 1}^{n_{seq}}\frac{\partial L}{\partial h_t}(1 - h_t^2)h_{t-1}^T, \text{ since }\frac{d}{dx} \text{tanh}(x)= 1-\text{tanh}^2(x). \end{align}\] Similarly, \[\begin{align}dW_{xh} &amp; = \sum_{t = 1}^{n_{seq}}\frac{\partial L}{\partial h_t}(1 - h_t^2)x_{t}^T\\\ db_h &amp; = \sum_{t = 1}^{n_{seq}}\frac{\partial L}{\partial h_t}(1 - h_t^2). \end{align}\] Now, it remains to compute \[\begin{align}\frac{\partial L}{\partial h_t} &amp; = \frac{\partial L}{\partial y_t}\frac{\partial y_t}{\partial h_t} + \frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_t} \\\ &amp; = \frac{\partial l_t}{\partial y_t}\frac{\partial y_t}{\partial h_t} + \frac{\partial L}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_{t+1}^{raw}} \frac{\partial h_{t+1}^{raw}}{\partial h_t}\\\ &amp; = W_{hy}^Tdy_t + \frac{\partial L}{\partial h_{t+1}}(1-h_{t+1}^2)\frac{\partial h_{t+1}^{raw}}{\partial h_t} \\\ &amp; = W_{hy}^Tdy_t + W_{hh}^T(1-h_{t+1}^2)\frac{\partial L}{\partial h_{t+1}}. \end{align}\]</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre>    <span class="k">def</span> <span class="nf">backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        
        <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dWhh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Why</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Whh</span><span class="p">)</span>
        <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">by</span><span class="p">)</span>
        <span class="n">dhnext</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">)):</span>
            <span class="n">dy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">dy</span><span class="p">[</span><span class="n">targets</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">=</span>  <span class="n">dy</span><span class="p">[</span><span class="n">targets</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">dWhy</span> <span class="o">=</span> <span class="n">dWhy</span> <span class="o">+</span> <span class="n">dy</span> <span class="o">@</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
            <span class="n">dby</span> <span class="o">=</span> <span class="n">dby</span> <span class="o">+</span> <span class="n">dy</span>
            <span class="n">dh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Why</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dy</span> <span class="o">+</span> <span class="n">dhnext</span>
            <span class="n">dhraw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dh</span>
            <span class="n">dbh</span> <span class="o">=</span> <span class="n">dbh</span> <span class="o">+</span> <span class="n">dhraw</span>
            <span class="n">dWxh</span> <span class="o">=</span> <span class="n">dWxh</span> <span class="o">+</span> <span class="n">dhraw</span> <span class="o">@</span> <span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
            <span class="n">dWhh</span> <span class="o">=</span> <span class="n">dWhh</span> <span class="o">+</span> <span class="n">dhraw</span> <span class="o">@</span> <span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
            <span class="n">dhnext</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dhraw</span>
        <span class="k">for</span> <span class="n">dpara</span> <span class="ow">in</span> <span class="p">[</span><span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dby</span><span class="p">,</span> <span class="n">dbh</span><span class="p">]:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">dpara</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">dpara</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span>
</pre></table></code></div></div><h2 id="an-example">An example</h2><p>Andrej Karpathy provides a small <a href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt">Shakespeare data set</a>. We set <code class="highlighter-rouge">n_h = 100</code> and <code class="highlighter-rouge">seq_length = 25</code> in the vanilla RNN. After 50000 iterations, the loss function can not furthur decrease. <img src="/assets/img/sample/loss_20200124.png" alt="fdr" width="400" class="center" /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
</pre><td class="rouge-code"><pre>   <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">char_to_int</span><span class="p">,</span> <span class="n">int_to_char</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">):</span>
        
        <span class="n">iter_num</span><span class="p">,</span> <span class="n">position</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_x</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">)</span>        
        
        <span class="k">while</span> <span class="n">iter_num</span> <span class="o">&lt;=</span> <span class="n">max_iter</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">iter_num</span><span class="o">%</span><span class="mi">1000</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="k">print</span><span class="p">(</span><span class="n">iter_num</span><span class="p">)</span>
            <span class="c1">## reset the rnn after an epoch
</span>            <span class="k">if</span> <span class="n">position</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="ow">or</span> <span class="n">iter_num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
                <span class="n">hprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_h</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">position</span> <span class="o">=</span> <span class="mi">0</span>
                
            <span class="c1">## chars to int
</span>            <span class="n">input_bacth</span> <span class="o">=</span> <span class="p">[</span><span class="n">char_to_int</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">[</span><span class="n">position</span><span class="p">:</span><span class="n">position</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">]]</span>
            <span class="n">target_bacth</span> <span class="o">=</span> <span class="p">[</span><span class="n">char_to_int</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">[</span><span class="n">position</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">position</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
            <span class="n">position</span> <span class="o">=</span> <span class="n">position</span> <span class="o">+</span> <span class="n">seq_length</span>
            <span class="c1">## forward_pass
</span>            <span class="n">loss</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_pass</span><span class="p">(</span><span class="n">input_bacth</span><span class="p">,</span> <span class="n">target_bacth</span><span class="p">,</span> <span class="n">hprev</span><span class="p">)</span>
            <span class="n">loss_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.999</span> <span class="o">+</span> <span class="n">loss</span> <span class="o">*</span> <span class="mf">0.001</span><span class="p">)</span>
            <span class="c1">## backpropagation
</span>            <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backpropagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">target_bacth</span><span class="p">)</span>
            <span class="c1">## adagrad upate
</span>            <span class="bp">self</span><span class="o">.</span><span class="n">update_para</span><span class="p">(</span><span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">)</span>
            <span class="n">hprev</span> <span class="o">=</span> <span class="n">h</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            
            <span class="n">iter_num</span> <span class="o">=</span> <span class="n">iter_num</span> <span class="o">+</span> <span class="mi">1</span>
            
        <span class="c1">## make a sample after training
</span>        <span class="n">sample_ix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_sample</span><span class="p">(</span><span class="n">hprev</span><span class="p">,</span> <span class="n">target_bacth</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">200</span><span class="p">)</span>
        <span class="n">sample_char</span> <span class="o">=</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">int_to_char</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="n">sample_ix</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">loss_list</span><span class="p">,</span> <span class="n">sample_char</span>

            
    <span class="k">def</span> <span class="nf">make_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hprev</span><span class="p">,</span> <span class="n">seed_ix</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="s">"""
        sample a length n sequence from the model
        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span><span class="p">[</span><span class="n">seed_ix</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">ixes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hprev</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bh</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Why</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">by</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_x</span><span class="p">),</span> <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">ixes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ixes</span>  
</pre></table></code></div></div><p>A length 200 sample is generated from the trained RNN.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'input.txt'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">words_size</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="n">char_to_int</span> <span class="o">=</span> <span class="p">{</span><span class="n">ch</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>
<span class="n">int_to_char</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>

<span class="n">rnn</span> <span class="o">=</span> <span class="n">vanillaRNN</span><span class="p">(</span><span class="n">n_x</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_h</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-1</span><span class="p">)</span>
<span class="n">loss_list</span><span class="p">,</span> <span class="n">sample_char</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">char_to_int</span><span class="p">,</span> <span class="n">int_to_char</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">50000</span><span class="p">)</span>
</pre></table></code></div></div><div class="highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>'kmmatody: nomels bake tho pav.\n.\n\nM:\nAtw: and I; thou onsel swere, lo! meroses ssseme noke shy ust but ker, woncter id imire ghy.\n\nWhat Thes hereth:\nIss:\nDrou wort, netesteme here to whont toy,\nAll My'
</pre></table></code></div></div></div><div class="post-tail text-muted"><div class="mb-4"> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >Deep learning</a></div></div></div><!-- The related posts of current post. Placed in the bottom of every single post. © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-4 mb-4 pb-3"><h3 class="pt-2 mt-1 mb-4" data-toc-skip>Related Posts</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Approximate-Elliptic-Paraboloid-by-Relu-nets/"><div class="card-body"> <span class="timeago small"> Jan 14, 2020 <i class="hidden">2020-01-14T23:34:00-08:00</i> </span><h3 class="pt-0 mt-2 mb-3" data-toc-skip>Approximating Elliptic Paraboloid by Relu nets</h3><div class="text-muted small"><p>If we regard neural net models as nonparametric approximators of continuous functions, several works are supporting the validity of this method. In particular, in reference 1, it has been proved...</p></div></div></a></div><div class="card"> <a href="/posts/Theoertical-investigation-of-batch-normalization/"><div class="card-body"> <span class="timeago small"> Jan 17, 2020 <i class="hidden">2020-01-17T08:25:00-08:00</i> </span><h3 class="pt-0 mt-2 mb-3" data-toc-skip>Theoretical investigation of batch normalization</h3><div class="text-muted small"><p>In last post, models with batch normalization have show advantages by a large margin. In this post, we will explore the following questions: How does batch normalization work? How is batch no...</p></div></div></a></div></div></div><div class="post-pager d-flex justify-content-between"> <a href="/posts/Theoertical-investigation-of-batch-normalization/" class="btn btn-outline-primary"> <i class="fas fa-angle-left mr-1"></i> OLDER POST </a> <a href="javascript:;" class="btn btn-outline-primary disabled"> NEWER POST <i class="fas fa-angle-right ml-1"></i> </a></div><!-- The Disqus lazy loading. Powered by: https://osvaldas.info/lazy-loading-disqus-comments © 2019 Cotes Chung MIT License --><div id="disqus" class="pt-2 pb-4"><p class="font-italic text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//LucyLiu.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'http://localhost:4000/posts/Backpropagation-of-a-vanilla-RNN/'; this.page.identifier = '/posts/Backpropagation-of-a-vanilla-RNN/'; } }; $.disqusLoader('#disqus', options); </script></div></div><!-- The Pannel on right side (Desktop views) © 2017-2019 Cotes Chung MIT License --><div id="panel-wrap" class="col-xl-3 pl-2 topbar-down"><div class="access"><div id="access-lastmod" class="post mb-4""><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Theoertical-investigation-of-batch-normalization/">Theoretical investigation of batch normalization</a></li><li><a href="/posts/Some-useful-operations-in-MySQL-median-level/">Some useful operations in MySQL--Median level</a></li><li><a href="/posts/Multiple-comparisons/">Multiple comparisons</a></li><li><a href="/posts/Sort-multiple-variables/">Sort multiple variables</a></li><li><a href="/posts/Some-useful-operations-in-MySQL-easy-level/">Some useful operations in MySQL--Easy level</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/manifold/">Manifold</a> <a class="post-tag" href="/tags/embedding/">Embedding</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/sql/">SQL</a> <a class="post-tag" href="/tags/recursion/">Recursion</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/statistics/">Statistics</a> <a class="post-tag" href="/tags/dynamic-programming/">Dynamic Programming</a> <a class="post-tag" href="/tags/divide&conquer/">Divide&Conquer</a> <a class="post-tag" href="/tags/directed-graph/">Directed Graph</a></div></div></div><div id="toc-wrap" class="pl-0 pr-4"><h3 data-toc-skip class="pl-3 pt-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><!-- The Footer © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="copyright"><p class="mb-0"> © 2019-2020<a href="https://twitter.com/LucyLiu84049511" class="ml-1">Lucy Liu</a>. <br>Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> & <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a>, hosted on <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.</p></div><div class="license"><p class="mb-0"> The blog posts on this site are licensed under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p></div></div></footer></div><!-- The Search results © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrap"><div class="row justify-content-center bg-white"><div class="col-12 col-md-12 col-lg-11 col-xl-9 pl-xl-5 pr-xl-5 pb-5 mt-3 mb-3"><h2 class="mt-3 pt-3 ml-3 ml-md-5 ml-lg-0" data-toc-skip>Search Results</h2><div class="post-content ml-1 ml-md-5 ml-lg-0"><ul id="search-results" ></ul></div></div></div></div></div><div id="mask"></div><!-- The Search © 2017-2019 Cotes Chung MIT License --> <script src="/assets/lib/simple-jekyll-search-1.7.1.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/search.json' }) </script> <a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a>
