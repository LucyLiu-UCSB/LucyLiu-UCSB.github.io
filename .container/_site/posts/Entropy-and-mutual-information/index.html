<!DOCTYPE html><html lang="en"> <!-- The Head © 2017-2019 Cotes Chung MIT License --><head><title>Entropy and mutual information | Lucy's Blog</title><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v3.8.6" /><meta property="og:title" content="Entropy and mutual information" /><meta name="author" content="Lucy Liu" /><meta property="og:locale" content="en_US" /><meta name="description" content="In statistical community, one of the primary estimation methods is maximum log-likelihood estimation (MLE). However, in machine learning/engineering, log-likelihood function has been renamed as cross entropy. This post is a summary of the chapter 2 in Elements of Information Theory." /><meta property="og:description" content="In statistical community, one of the primary estimation methods is maximum log-likelihood estimation (MLE). However, in machine learning/engineering, log-likelihood function has been renamed as cross entropy. This post is a summary of the chapter 2 in Elements of Information Theory." /><link rel="canonical" href="http://localhost:4000/posts/Entropy-and-mutual-information/" /><meta property="og:url" content="http://localhost:4000/posts/Entropy-and-mutual-information/" /><meta property="og:site_name" content="Lucy’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-01-31T15:58:00-08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Entropy and mutual information" /><meta name="twitter:site" content="@LucyLiu84049511" /><meta name="twitter:creator" content="@Lucy Liu" /><meta name="google-site-verification" content="828U5Odxo38lGZ7ca6xyIc_r3oCovf3Xsr-XyShs3Ek" /> <script type="application/ld+json"> {"description":"In statistical community, one of the primary estimation methods is maximum log-likelihood estimation (MLE). However, in machine learning/engineering, log-likelihood function has been renamed as cross entropy. This post is a summary of the chapter 2 in Elements of Information Theory.","@type":"BlogPosting","headline":"Entropy and mutual information","dateModified":"2020-01-31T15:58:00-08:00","datePublished":"2020-01-31T15:58:00-08:00","url":"http://localhost:4000/posts/Entropy-and-mutual-information/","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/Entropy-and-mutual-information/"},"author":{"@type":"Person","name":"Lucy Liu"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/main.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script> document.jQuery || document.write('<script src="/assets/lib/jquery-3.4.1.min.js"><\/script>'); </script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.15.0/dist/umd/popper.min.js" integrity="sha256-fTuUgtT7O2rqoImwjrhDgbXTKUwyxxujIMRIK7TbuNU=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha256-5+02zu5UULQkO7w1GIr6vftCgMfFdZcAHeDtFnKZsBs=" crossorigin="anonymous"></script> <script src="/assets/js/dist/commons.js" async></script> <script src="/assets/js/dist/timeago.min.js"></script><link rel="stylesheet" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/lib/bootstrap-toc-1.0.1/bootstrap-toc.min.css" /> <script src="/assets/lib/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script> <script src="/assets/js/dist/toc.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar © 2017-2019 Cotes Chung MIT License --><div id="nav-wrap"><div id="profile-wrap" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/Lucy.png"></img> </a></div><div class="profile-text mt-3"><div id="site-title"> <a href="/">Lucy's Blog</a></div><div id="site-subtitle" class="font-italic">Learning for the sake of Learning</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-3 mr-4 hidden"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-3 mr-4 hidden"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-3 mr-4 hidden"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-3 mr-4 hidden"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-3 mr-4 hidden"></i> <span>ABOUT</span> </a></li></ul></div><div class="contact d-flex justify-content-around mt-4"> <a href="https://github.com/LucyLiu-UCSB" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/LucyLiu84049511" target="_blank"> <i class="fab fa-twitter"></i> </a> <a href="javascript:window.open('mailto:' + ['xiliu','ucsb.edu'].join('@'))"> <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" target="_blank"> <i class="fas fa-rss"></i> </a></div></div><div id="main-wrap"> <!-- The Top Bar © 2017-2019 Cotes Chung MIT License --><div id="topbar" class="bg-white row justify-content-center topbar-down"><div id="topbar-main" class="d-flex h-100 align-items-center justify-content-between col-12 col-md-12 col-lg-11 col-xl-11 pl-md-2 pr-md-2 pl-lg-2 pr-lg-2"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Entropy and mutual information</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrap"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> </span> <a href="javascript:;">Cancel</a></div></div><div id="main"><div class="row justify-content-center bg-white"> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <!-- Define the liquid date formats. © 2019 Cotes Chung Published under the MIT License --><div class="col-12 col-lg-11 col-xl-8"><div id="post-wrap" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4 pl-xl-3"><div class="post"><h1 data-toc-skip>Entropy and mutual information</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago" data-toggle="tooltip" title="Fri, Jan 31, 2020, 3:58 PM -0800"> Jan 31, 2020 <i class="hidden">2020-01-31T15:58:00-08:00</i> </span> on <a href='/categories/technical-tools/'>Technical Tools</a>, <a href='/categories/statistics/'>Statistics</a></div></div><div class="post-content"><p>In statistical community, one of the primary estimation methods is maximum log-likelihood estimation (MLE). However, in machine learning/engineering, log-likelihood function has been renamed as cross entropy. This post is a summary of the chapter 2 in <a href="https://ebookcentral.proquest.com/lib/ucsb-ebooks/detail.action?docID=266952#goto_toc">Elements of Information Theory</a>.</p><h2 id="an-example">An example</h2><p>We start with an example. Let X be a random variable taking values a, b, c, d with probability 1/2, 1/4, 1/8, 1/8 respectively. We would like to know <strong>the minimum expected number of binary questions required to determine the value of X.</strong> An efficient asking strategy will be asking 1) is X = a? 2) is X = b? 3) is X = c? sequentially. Therefore, the number of questions required is \[1\times 1/2 + 2\times 1/4 + 3\times(1/8+1/8) = 1.75 = H(X).\]</p><p>What if the probability distribution changes to 1/4, 1/4, 1/4 and 1/4? Then the number of questions required is \[1\times 1/4 + 2\times 1/4 + 3\times(1/4+1/4) = 2.25.\]</p><p>Actually, the minimum expected number of binary questions required to determine \(X\) lies between \(H(X)\) and \(H(X) + 1\). Entropy, \(H(X)\), is a measure of the uncertainty of a random variable; it is a measure of the amount of information required on the average to describe the random variable.</p><h2 id="definition-of-entropy">Definition of Entropy</h2><p><strong>Definition:</strong> The <strong><em>entropy</em></strong> \(H(X)\) of a discrete random variable \(X\) is defined by \[H(X) \text{ or } H(p) = -\Sigma_{x\in\mathcal{X}} p(x) \log p(x) = E_p\log \frac{1}{p(X)} \geq 0.\] The log is to the base 2 and entropy is expressed in <em>bits</em>. Note that \(0\log 0 = 0\). If we denote the \(H_b(X) = -\Sigma_{x\in\mathcal{X}} p(x) \log_b p(x)\), we have \(H_b(X) = (\log_ba)H_a(X)\).</p><p><strong>Definition:</strong> The <strong><em>joint entropy</em></strong> \(H(X, Y)\) of a pair of discrete random variable \(X, Y\) with a joint distribution \(p(x, y)\) is defined as \[H(X, Y) = -\Sigma_{x\in\mathcal{X}}\Sigma_{y\in\mathcal{Y}}p(x, y)\log p(x, y) = -E\log p(X, Y).\]</p><p><strong>Definition:</strong> If \((X, Y)\sim p(x, y)\), the <strong><em>conditional entropy</em></strong> \(H(Y|X)\) is defined as \[\begin{align} H(Y|X) &amp; =\Sigma_{x\in\mathcal{X}}p(x)H(Y|X = x)\\\ &amp; = -\Sigma_{x\in\mathcal{X}}p(x)\Sigma_{y\in\mathcal{Y}}p(y|x)\log p(y|x)\\\ &amp; = -\Sigma_{x\in\mathcal{X}}\Sigma_{y\in\mathcal{Y}}p(x, y)\log p(y|x)\\\ &amp; = -E\log p(Y|X). \end{align}\]</p><p><strong>Definition:</strong> The <strong><em>relative entropy</em></strong> or <strong><em>Kullback-Leibler distance</em></strong> between two probability mass functions \(p(x)\) and \(q(x)\) is defined as \[D(p||q) = \sum_{x\in\mathcal{X}}p(x)\log \frac{p(x)}{q(x)} = E_P\log\frac{p(X)}{q(X)}.\] We use the convention that \(0\log \frac{0}{q} = 0\), \(0\log\frac{0}{q} = 0\) and \(p\log\frac{p}{0} = \infty\). It is not a true distance between distributions since it is not symmetric and does not satisfy the triangle inequality. \(D(p||q)\) is a measure of the inefficiency of assuming that the distribution is \(q\) when the true distribution is \(p\).</p><p><strong>Definition:</strong> Consider two random variables \(X\) and \(Y\) with a joint probability mass function \(p(x, y)\) and marginal probability mass functions \(p(x)\) and \(p(y)\). The <strong><em>mutual information</em></strong> \(I(X;Y)\) is the relative entropy between the joint distribution and the product distribution \(p(x)p(y):\) \[\begin{align} I(X;Y) &amp; = \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x, y)\log\frac{p(x, y)}{p(x)p(y)}\\\ &amp; = D(p(x, y)||p(x)p(y))\\\ &amp; = E_{p(x, y)}\log \frac{p(X, Y)}{p(X)p(Y)}. \end{align}\] Mutual information measures the amount of information that one random variable contains about another random variable. It is the reduction in the uncertainty of one random variable due to the knowledge of the other.</p><p><img src="/assets/img/sample/entropy_20200131.png" alt="fdr" width="400" class="center" /></p><h2 id="relationship-and-theorems">relationship and theorems</h2><ol><li><strong>Chain rule</strong> \[\begin{align} H(X, Y) &amp; = H(X) + H(Y|X)\\\ H(X, Y|Z) &amp; = H(X|Z) + H(Y|X, Z)\\\ H(X_1, X_2, \dots, X_n) &amp; = \Sigma_{i = 1}^n H(X_i|X_{i-1}, \ldots, X_1) \end{align}\] Note that \(H(Y|X)\neq H(X|Y)\) but \(H(X) - H(X|Y) = H(Y) - H(Y|X).\)</li><li><strong>X says as much about as Y as Y says about X</strong> \[\begin{align} I(X;Y) &amp;= H(X) - H(X|Y) = H(X) + H(Y) -H(X, Y) \\\ I(X;Y) &amp;= H(Y) - H(Y|X)\\\ I(X;X) &amp;= H(X) - H(X|X) = H(X) \end{align}\]</li><li><strong>infomation inequality</strong> \[D(p||q)\geq 0 \] with equality if and only if \(p(x) = q(x)\) for all \(x\). (this can be proved by Jensen’s inequality since log(x) is concave.)</li><li><strong>Nonnegativity of mutual information</strong> For any two random variable, \(X, Y\), \[I(X;Y)\geq 0 \] with equality if and only if \(X\) and \(Y\) are independent.</li><li><strong>maximum entropy</strong> \[H(X)\leq\log|\mathcal{X}|,\] where \(|\mathcal{X}|\) denotes the number of elements in the range of \(X\), with equality if and only if \(X\) has a uniform distribution over \(\mathcal{X}\).</li><li><strong>Conditioning reduces entropy, information can’t hurt</strong> \[H(X|Y)\leq H(X)\] Intuitively, the theorem says that knowing another random variable \(Y\) can only reduce the uncertanty in \(X\). But this is only true on the average. Specifically, \(H(X|Y= y)\) may be greater than or less than or equal to \(H(X)\).</li><li><strong>Independence bound on entropy</strong> <br /> Let \(X_1, X_2, \ldots, X_n\) be drawn according to \(p(x_1, x_2, \ldots, x_n)\). Then \[H(X_1, X_2, \ldots, X_n)\leq \sum_{i = 1}^n H(X_i)\] with equality if and only if the \(X_i\) are independent.</li></ol><h2 id="concavity-of-entropy">Concavity of entropy</h2><p>First, we introduce the <strong>log sum inequality</strong>. <br /> For nonnegative numbers, \(a_1, a_2, \ldots, a_n\) and \(b_1, b_2, \ldots, b_n\), \[\sum_{i = 1}^n a_i\log\frac{a_i}{b_i}\geq \left(\sum_{i=1}^n a_i \right) \log \frac{\Sigma_{i = 1}^n{a_i}}{\Sigma_{i = 1}^nb_i}\] with equality if and only if \(\frac{a_i}{b_i} = \) const.</p><p>By using the log sum inequality, we can prove the following theorems.</p><ol><li><strong>Convexity of relative entropy</strong> <br /> \(D(p||q)\) is convex in the pair \((p, q)\); that is, if \((p_1, q_1)\) and \((p_2, q_2)\) are two pairs of probability mass functions, then \[D(\lambda p_1 + (1-\lambda)p_2 || \lambda q_1 + (1-\lambda)q_2) \leq \lambda D(p_1||q_1) + (1-\lambda)D(p_2||q_2)\] for all \(0\leq \lambda\leq 1\).</li><li><strong>Concavity of entropy</strong> <br /> \(H(p)\) is a concave function of \(p\) since \(H(p) = \log|\mathcal{X}| - D(p||u),\) where \(u\) is the uniform distribution on \(|\mathcal{X}|\) outcomes.</li><li><strong>Concavity and convexity of mutual information</strong> <br /> Let \((X, Y)\sim p(x,y) = p(x)p(y|x)\). The mutual information \(I(X; Y)\) is a concave function of \(p(x)\) for fixed \(p(y|x)\) and a convex function of \(p(y|x)\) for fixed \(p(x)\).</li></ol><h2 id="fanos-inequality">Fano’s inequality</h2><p>Fano’s inequality relates the probability of error in guessing the random variable \(X\) to its conditional entropy \(H(X|Y)\). The conditional entropy of a random variable \(X\) given another random variable \(Y\) is zero if and only if \(X\) if a function of \(Y\) so that \(X\) become deterministic given \(Y\). Hence we can estimate \(X\) from \(Y\) with zero probability of error if and only if \(H(X|Y) = 0.\)</p><p>Extending this argument, we expect to be able to estimate \(X\) with a low probability of error only if the conditional entropy \(H(X|Y)\) is small. Fano’s inequality quantifies this idea. Suppose we use a function \(g(Y)\) as the estimator \(\hat{X}\) to estimate \(X\) and define the probability of error \[P_e = \text{Pr} (\hat{X}\neq X).\] <strong>Theorem (Fano’s inequality)</strong> For any estimator \(\hat{X}\) as a function of \(Y\), we have \[H(P_e) + P_e\log|\mathcal{X}| \geq H(X|\hat{X}) \geq H(X|Y).\] This inequality can be weakened to \[1 + P_e\log|\mathcal{X}| \geq H(X|Y).\] <strong>Corollary</strong> For any two random variable \(X\) and \(Y\), let \(p = \text{Pr} (X\neq Y)\). \[H(p) + p\log|\mathcal{X}|\geq H(X|Y).\] <strong>Theorem (information in independent copy)</strong> If \(X\) and \(X’\) are i.i.d with entropy \(H(X)\), \[\text{Pr}(X = X’) \geq 2^{-H(X)}.\] <strong>Corollary</strong> Let \(X, X’\) be independent with \(X\sim p(x), X’\sim r(x), x, x’\in \mathcal{X}.\) Then \[\begin{align} \text{Pr} (X = X’) &amp; \geq 2^{-H(p)-D(p||r)}\\\ \text{Pr} (X = X’) &amp; \geq 2^{-H(r) - D(r||p)}. \end{align}\]</p></div><div class="post-tail text-muted"><div class="mb-4"> <a href="/tags/statistics/" class="post-tag no-text-decoration" >Statistics</a></div></div></div><!-- The related posts of current post. Placed in the bottom of every single post. © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-4 mb-4 pb-3"><h3 class="pt-2 mt-1 mb-4" data-toc-skip>Related Posts</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Multiple-comparisons/"><div class="card-body"> <span class="timeago small"> Jan 6, 2020 <i class="hidden">2020-01-06T10:18:00-08:00</i> </span><h3 class="pt-0 mt-2 mb-3" data-toc-skip>Multiple comparisons</h3><div class="text-muted small"><p>In statistics, multiple comparisons/multiple hypothesis testing occurs when one considers a set of statistical inference questions simultaneously. To control the chance of making mistakes when the ...</p></div></div></a></div><div class="card"> <a href="/posts/Asterisk-in-Python/"><div class="card-body"> <span class="timeago small"> Dec 17, 2019 <i class="hidden">2019-12-17T00:00:00-08:00</i> </span><h3 class="pt-0 mt-2 mb-3" data-toc-skip>Asterisk in Python</h3><div class="text-muted small"><p>The use of * in python is related to the way of passing parameters to a function. First, in python, we have three types of parameters, Positional-or-Keyword Arguments, Positional-Only Parameters an...</p></div></div></a></div><div class="card"> <a href="/posts/Sort-multiple-variables/"><div class="card-body"> <span class="timeago small"> Dec 31, 2019 <i class="hidden">2019-12-31T14:41:00-08:00</i> </span><h3 class="pt-0 mt-2 mb-3" data-toc-skip>Sort multiple variables</h3><div class="text-muted small"><p>Usually, we are proficient at sorting the data frame/table by one variable. But there are cases that we need a second variable to break the ties. In this post, I will summarize how to do this in Py...</p></div></div></a></div></div></div><div class="post-pager d-flex justify-content-between"> <a href="/posts/Backpropagation-of-a-vanilla-RNN/" class="btn btn-outline-primary"> <i class="fas fa-angle-left mr-1"></i> OLDER POST </a> <a href="javascript:;" class="btn btn-outline-primary disabled"> NEWER POST <i class="fas fa-angle-right ml-1"></i> </a></div><!-- The Disqus lazy loading. Powered by: https://osvaldas.info/lazy-loading-disqus-comments © 2019 Cotes Chung MIT License --><div id="disqus" class="pt-2 pb-4"><p class="font-italic text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//LucyLiu.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'http://localhost:4000/posts/Entropy-and-mutual-information/'; this.page.identifier = '/posts/Entropy-and-mutual-information/'; } }; $.disqusLoader('#disqus', options); </script></div></div><!-- The Pannel on right side (Desktop views) © 2017-2019 Cotes Chung MIT License --><div id="panel-wrap" class="col-xl-3 pl-2 topbar-down"><div class="access"><div id="access-lastmod" class="post mb-4""><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Backpropagation-of-a-vanilla-RNN/">Backpropagation of a vanilla RNN</a></li><li><a href="/posts/Approximate-Elliptic-Paraboloid-by-Relu-nets/">Approximating Elliptic Paraboloid by Relu nets</a></li><li><a href="/posts/Theoertical-investigation-of-batch-normalization/">Theoretical investigation of batch normalization</a></li><li><a href="/posts/Multiple-comparisons/">Multiple comparisons</a></li><li><a href="/posts/Some-useful-operations-in-MySQL-median-level/">Some useful operations in MySQL--Median level</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/manifold/">Manifold</a> <a class="post-tag" href="/tags/embedding/">Embedding</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/statistics/">Statistics</a> <a class="post-tag" href="/tags/sql/">SQL</a> <a class="post-tag" href="/tags/recursion/">Recursion</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/dynamic-programming/">Dynamic Programming</a> <a class="post-tag" href="/tags/divide&conquer/">Divide&Conquer</a> <a class="post-tag" href="/tags/directed-graph/">Directed Graph</a></div></div></div><div id="toc-wrap" class="pl-0 pr-4"><h3 data-toc-skip class="pl-3 pt-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><!-- The Footer © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="copyright"><p class="mb-0"> © 2019-2020<a href="https://twitter.com/LucyLiu84049511" class="ml-1">Lucy Liu</a>. <br>Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> & <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a>, hosted on <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.</p></div><div class="license"><p class="mb-0"> The blog posts on this site are licensed under the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p></div></div></footer></div><!-- The Search results © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrap"><div class="row justify-content-center bg-white"><div class="col-12 col-md-12 col-lg-11 col-xl-9 pl-xl-5 pr-xl-5 pb-5 mt-3 mb-3"><h2 class="mt-3 pt-3 ml-3 ml-md-5 ml-lg-0" data-toc-skip>Search Results</h2><div class="post-content ml-1 ml-md-5 ml-lg-0"><ul id="search-results" ></ul></div></div></div></div></div><div id="mask"></div><!-- The Search © 2017-2019 Cotes Chung MIT License --> <script src="/assets/lib/simple-jekyll-search-1.7.1.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/search.json' }) </script> <a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a>
